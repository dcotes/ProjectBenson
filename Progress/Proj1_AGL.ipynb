{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Pandas version: 0.23.4\n",
      "Numpy version: 1.14.3\n"
     ]
    }
   ],
   "source": [
    "# Set up PANDAS\n",
    "\n",
    "% pylab inline\n",
    "# see https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import json\n",
    "\n",
    "print(\"Pandas version:\", pd.__version__)\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 125)\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = Farecard Data\n",
    "# df2 = Turnstile Data\n",
    "# df3 = Entrance Information\n",
    "# df1 = pd.read_csv(\n",
    "#     'http://web.mta.info/developers/data/nyct/fares/fares_180922.csv', header=[2]\n",
    "#     )\n",
    "df2 = pd.read_csv(\n",
    "    'http://web.mta.info/developers/data/nyct/turnstile/turnstile_180922.txt'\n",
    "    )\n",
    "#df1 = df1.rename(index=str,{\"10-T\": \"10 TRIP\", \"14-D\": \"14 DAY\", \"1-D\": \"1 DAY/FUNPASS\", \"2-T\": \"2 TRIP\", \"30-D\": \"30 DAY\", \"7-D\": \"7 DAY\", \"ADA\": \"AMERICANS WITH DISABLITIES ACT\", \"AFAS\": \"ADA FARECARD ACCESS SYSTEM\", \"EXP\": \"EXPRESS\", \"EZPAY\": \"EASY PAY\", \"FF\": \"FULL FARE\", \"MC\": \"METROCARD\", \"MR\": \"MAIL AND RIDE\", \"MTHLY\": \"MONTHLY\", \"RFM\": \"REDUCED FARE MEDIA\", \"RR\": \"RAIL ROAD\", \"SEN/DIS\": \"SENIOR CITIZEN/DISABLED\", \"SPEC\": \"SPECIAL\", \"TCMC\": \"TRANSIT CHECK METROCARD\", \"TKT\": \"TICKET\", \"UNL\": \"UNLIMITED\", \"XBUS\": \"EXPRESS BUS\", \"STUDENT\": \"STUDENT USAGE\"})\n",
    "df3 = pd.read_csv(\n",
    "    'http://web.mta.info/developers/data/nyct/subway/StationEntrances.csv'\n",
    "    )\n",
    "### Test\n",
    "df4 = pd.read_csv(\n",
    "    'http://web.mta.info/developers/data/nyct/subway/StationEntrances.csv'\n",
    "    )\n",
    "\n",
    "df2['DATETIME'] = df2['DATE'] + ' ' + df2['TIME']\n",
    "df2['DATETIME'] = pd.to_datetime(df2['DATETIME'],format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "# Clean-up labels\n",
    "df2.rename(columns={'EXITS                                                               ': 'EXIT'}, inplace=True)\n",
    "\n",
    "### Clean: Convert the 4 series to string if not null \n",
    "df3['Route_8'] = df3['Route_8'].apply(lambda x: \"{:.0f}\".format(x) if not pd.isnull(x) else x)\n",
    "df3['Route_9'] = df3['Route_9'].apply(lambda x: \"{:.0f}\".format(x) if not pd.isnull(x) else x)\n",
    "df3['Route_10'] = df3['Route_10'].apply(lambda x: \"{:.0f}\".format(x) if not pd.isnull(x) else x)\n",
    "df3['Route_11'] = df3['Route_11'].apply(lambda x: \"{:.0f}\".format(x) if not pd.isnull(x) else x)\n",
    "\n",
    "### Clean: Remove all whitespaces for the 7 series\n",
    "df3['Route_1'] = df3.Route_1.str.strip()\n",
    "df3['Route_2'] = df3.Route_2.str.strip()\n",
    "df3['Route_3'] = df3.Route_3.str.strip()\n",
    "df3['Route_4'] = df3.Route_4.str.strip()\n",
    "df3['Route_5'] = df3.Route_5.str.strip()\n",
    "df3['Route_6'] = df3.Route_6.str.strip()\n",
    "df3['Route_7'] = df3.Route_7.str.strip()\n",
    "\n",
    "### Concatenate lines (Routes) and create 'CnctRte'\n",
    "df3['CnctRte'] = df3['Route_1'].fillna('') + df3['Route_2'].fillna('') + df3['Route_3'].fillna('') + df3['Route_4'].fillna('') + df3['Route_5'].fillna('') + df3['Route_6'].fillna('') + df3['Route_7'].fillna('') + df3['Route_8'].fillna('') + df3['Route_9'].fillna('') + df3['Route_10'].fillna('') + df3['Route_11'].fillna('')\n",
    "# df3['CnctRte'] = set(df3['Route_1'].fillna(''), df3['Route_2'].fillna(''), df3['Route_3'].fillna(''), df3['Route_4'].fillna(''), df3['Route_5'].fillna(''), df3['Route_6'].fillna(''), df3['Route_7'].fillna(''), df3['Route_8'].fillna(''), df3['Route_9'].fillna(''), df3['Route_10'].fillna(''), df3['Route_11'].fillna(''))\n",
    "\n",
    "\n",
    "# def alert(c):\n",
    "#   if c['used'] == 1.0:\n",
    "#     return 'Full'\n",
    "#   elif c['used'] == 0.0:\n",
    "#     return 'Empty'\n",
    "#   elif 0.0 < c['used'] < 1.0:\n",
    "#     return 'Partial'\n",
    "#   else:\n",
    "#     return 'Undefined'\n",
    "# df3['Route_12'] = df3.apply(lambda x: \"{:.0f}\".format(x) if not pd.isnull(x) else x)\n",
    "\n",
    "\n",
    "df2['LINENAME'] = df2['LINENAME'].apply(lambda x: ''.join(sorted(set(x))))\n",
    "df3['CnctRte'] = df3['CnctRte'].apply(lambda x: ''.join(sorted(set(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 AV/59 ST\n",
      "57 ST-7 AV\n",
      "34 ST-HERALD SQ\n",
      "8 ST-NYU\n",
      "WHITEHALL S-FRY\n",
      "JAY ST-METROTEC\n",
      "ATL AV-BARCLAY\n",
      "4AV-9 ST\n",
      "BAY RIDGE-95 ST\n",
      "BAY 50 ST\n",
      "CONEY IS-STILLW\n",
      "W 8 ST-AQUARIUM\n",
      "1 AV\n",
      "NEW LOTS\n",
      "CANARSIE-ROCKAW\n",
      "HOWARD BCH JFK\n",
      "75 ST-ELDERTS\n"
     ]
    }
   ],
   "source": [
    "# Import Nominatim from geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"my-application\")\n",
    "\n",
    "# Create a list of station to source the search on Google API\n",
    "mta_stations = list(df2['STATION'].unique())\n",
    "# Create a dataframe to collect search results\n",
    "stations_loc = pd.DataFrame(columns = ['STATION', 'LAT', 'LONG'])\n",
    "locs = pd.DataFrame(data=None)\n",
    "\n",
    "\n",
    "######## Below uses geopy Nominatim. For Google API see below.\n",
    "for STATION in mta_stations:\n",
    "    try:\n",
    "        search_criteria = STATION + ' New York, NY'\n",
    "        location = geolocator.geocode(search_criteria)\n",
    "        lat = location.latitude\n",
    "        long = location.longitude\n",
    "        locs = locs.append(pd.Series((STATION, lat, long)), ignore_index=True)\n",
    "    except:\n",
    "        print(STATION)\n",
    "\n",
    "# STATION = 'LIVONIA AV'\n",
    "# search_criteria = STATION + ' station, New York, NY'\n",
    "# location = geolocator.geocode(search_criteria)\n",
    "# lat = location.latitude\n",
    "# long = location.longitude\n",
    "# locs = locs.append(pd.Series((STATION, lat, long)), ignore_index=True)\n",
    "# locs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JUNCTION BLVD\n"
     ]
    }
   ],
   "source": [
    "# Set variables for the API\n",
    "locs = pd.DataFrame(data=None)\n",
    "gmaps_base_url = 'https://maps.googleapis.com/maps/api/geocode/json?'\n",
    "api_key = 'AIzaSyBrSMdjoUWrRflbtMYx3LDcXgzXR_puIF0'\n",
    "scontext = None\n",
    "\n",
    "# Create a list of station to source the search on Google API\n",
    "mta_stations = list(df2['STATION'].unique())\n",
    "# Create a dataframe to collect search results\n",
    "stations_loc = pd.DataFrame(columns = ['STATION', 'LAT', 'LONG'])\n",
    "\n",
    "# search_criteria = {'address': 'TIMES SQ-42 ST' + 'Subway station, New York, NY',\n",
    "#                    'key' : api_key\n",
    "#                    }\n",
    "# url = gmaps_base_url + urllib.parse.urlencode(search_criteria)\n",
    "# uh = urllib.request.urlopen(url, context=scontext)\n",
    "# data = uh.read()\n",
    "# js = json.loads(data)\n",
    "# lat = js['results'][0]['geometry']['location']['lat']\n",
    "# long = js['results'][0]['geometry']['location']['lng']\n",
    "# locs = locs.append(pd.Series((station, lat, long)), ignore_index=True)\n",
    "\n",
    "# Search\n",
    "for station in mta_stations:\n",
    "    try:\n",
    "        search_criteria = {'address': station + 'Subway station, New York, NY',\n",
    "                           'key' : api_key\n",
    "                           }\n",
    "        url = gmaps_base_url + urllib.parse.urlencode(search_criteria)\n",
    "        uh = urllib.request.urlopen(url, context=scontext)\n",
    "        data = uh.read()\n",
    "        js = json.loads(data)\n",
    "        lat = js['results'][0]['geometry']['location']['lat']\n",
    "        long = js['results'][0]['geometry']['location']['lng']\n",
    "        locs = locs.append(pd.Series((station, lat, long)), ignore_index=True)\n",
    "    except:\n",
    "        print(station)\n",
    "\n",
    "# Search\n",
    "# for STATION in mta_stations:\n",
    "#     try:\n",
    "#         search_criteria = {STATION + ' station, New York, NY'}\n",
    "#         location = geolocator.geocode(search_criteria)\n",
    "#         lat = location.latitude\n",
    "#         long = location.longitude\n",
    "#         locs = locs.append(pd.Series((station, lat, long)), ignore_index=True)\n",
    "#     except:\n",
    "#         print(station)\n",
    "\n",
    "# locs.to_frame()\n",
    "\n",
    "# errors = {'CONEY IS-STILLW' : 11224,\n",
    "#           'JFK JAMAICA CT1' : 11435,\n",
    "#           'HOYT-SCHER' : 11201,\n",
    "#           'AQUEDUCT N.COND' : 11417,\n",
    "#           'KEW GARDENS' : 11415,\n",
    "#           'NEWARK BM BW' : '07102',\n",
    "#           'JAMAICA VAN WK' : 11418,\n",
    "#           'MORISN AV/SNDVW' : 10472,\n",
    "# # }\n",
    "\n",
    "# for station, zip_code in errors.iteritems():\n",
    "#     zips = zips.append(pd.Series((station, zip_code)), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.STATION.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs.to_csv('LongLat.csv', sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3.Station_Name==\"Fulton St\"].head(25)\n",
    "pd.to_datetime(df2['TIME'],format = \"%H:%M:%S\").dt.time\n",
    "\n",
    "\n",
    "\n",
    "# df2.sample(25, random_state=42)\n",
    "# df2[(df2.LINENAME=='NQR')].groupby('DIVISION').count()\n",
    "# df2.filter(df2.LINENAME.like('')).collect()\n",
    "\n",
    "\n",
    "\n",
    "#df2[(df2.UNIT=='R318')&(df2.STATION!='FULTON ST')].sort_values(['DATE','TIME'], ascending=True)\n",
    "#df2[(df2.UNIT=='R318')].sort_values(['DATE','TIME'], ascending=True)\n",
    "\n",
    "\n",
    "\n",
    "# df2st = df2.groupby('STATION', as_index=False)['UNIT'].count()\n",
    "# df3st = df3.groupby('Station_Name', as_index=False)['Corner'].count()\n",
    "# dfj = pd.merge(df2st, df3st, how='left', left_on='STATION', right_on='Station_Name')\n",
    "\n",
    "df2st = df2.groupby('STATION', as_index=False)['UNIT'].count()\n",
    "df3st = df3.groupby('Station_Name', 'Station_Latitude', 'Station_Longitude', as_index=False)['Corner'].count()\n",
    "dfj = pd.merge(df2st, df3st, how='inner', left_on='STATION', right_on='Station_Name').\n",
    "\n",
    "\n",
    "\n",
    "# df2.groupby('STATION')['STATION'].nunique()\n",
    "# df2.groupby(['DATETIME','STATION'])['ENTRIES','EXIT'].sum()\n",
    "# # df2.head(50)\n",
    "# #df2.sample(25, random_state=42)\n",
    "# df2['LNM'] = df2.LINENAME[:]\n",
    "# df2.sample(100)\n",
    "# .groupby('LINENAME').nunique()\n",
    "# df3.groupby('Cre').nunique()\n",
    "\n",
    "\n",
    "\n",
    "# df3['CnctRte'].append()\n",
    "\n",
    "# df3['key'] = df3['']\n",
    "\n",
    "# df3 = df3.drop(['CnctRte1', 'CnctRte2'], axis=1)\n",
    "# df3['CnctRte1'] = df3['Route_1'].fillna('') + df3['Route_2'].fillna('') + df3['Route_3'].fillna('') + df3['Route_4'].fillna('') + df3['Route_5'].fillna('') + df3['Route_6'].fillna('') + df3['Route_7'].fillna('') + df3['Route_8'].astype(str).fillna('') + df3['Route_9'].fillna('') + df3['Route_10'].astype(str).fillna('') + df3['Route_11'].astype(str).fillna('')\n",
    "# df3['CnctRte1'] = df3['Route_1', 'Route_2', 'Route_3', 'Route_4', 'Route_5', 'Route_6', 'Route_7', 'Route_8', 'Route_9', 'Route_10', 'Route_11'].astype(str).values.sum(axis=1)\n",
    "# df3[df3.Route_8.notnull()]\n",
    "# df3.columns\n",
    "# df3['Corner'] = df3['Corner'].apply(lambda x: \"{:.0f}\".format(x) if not pd.isnull(x) else x)\n",
    "df3.groupby(['Station_Name', 'Station_Latitude', 'Station_Longitude'], as_index=False)['Corner'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Station_Name          CnctRte       Division\n",
       "42nd St               1237ACENQRS   IND         9\n",
       "Times Square          1237ACEGNQRS  IRT         1\n",
       "                      1237ACENQRS   IRT         9\n",
       "Times Square-42nd St  1237ACENQRS   BMT         4\n",
       "Name: Corner, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.merge()\n",
    "df2.groupby(['STATION','LINENAME'])['LINENAME'].nunique().shape[0]\n",
    "# df2.groupby(['STATION','LINENAME'])['ENTRIES','EXIT'].sum()\n",
    "# df3.CnctRte.dtype\n",
    "# df3.CnctRte.sample(150)\n",
    "\n",
    "\n",
    "\n",
    "# df3[df3.Route_1=='B ']\n",
    "# df3.groupby(['Station_Name', 'Station_Latitude', 'Station_Longitude'], as_index=False)['Corner'].count()\n",
    "# df3.groupby(['CnctRte'])['CnctRte'].nunique()\n",
    "df3.groupby(['Station_Name','CnctRte'])['CnctRte'].nunique().shape[0]\n",
    "\n",
    "\n",
    "\n",
    "# edit distances\n",
    "# levinstein distance\n",
    "\n",
    "# df2[df2.STATION=='Rockaway Park-Beach 116th']\n",
    "# df2[(df2.LINENAME=='1237ACEGNRSW')&(df2.STATION=='42 ST-PORT AUTH')|(df2.LINENAME=='1237ACENQRSW')&(df2.STATION=='42 ST-PORT AUTH')].groupby(['STATION','LINENAME','DIVISION'])['UNIT'].count()\n",
    "df2[(df2.LINENAME=='1237ACEGNRSW')|(df2.LINENAME=='1237ACENQRSW')|(df2.LINENAME=='1237ACENQRS')].groupby(['STATION','LINENAME','DIVISION'])['UNIT'].count()# df2[df2.STATION=='42 ST-PORT AUTH'].groupby(['STATION','LINENAME','DIVISION'])['UNIT'].count()\n",
    "# df2.STATION.nunique()\n",
    "# df3.groupby(['Station_Name', 'Division', 'Station_Latitude', 'Station_Longitude'])['Corner'].count().reset_index()\n",
    "# df3.columns\n",
    "# sample(25, random_state=32)\n",
    "\n",
    "df3[(df3.CnctRte=='1237ACEGNQRS')|(df3.CnctRte=='1237ACENQRS')].groupby(['Station_Name','CnctRte','Division'])['Corner'].count()\n",
    "# df3[df3.Division=='N307']\n",
    "# df3[df3.CnctRte=='H']\n",
    "# df3.Station_Name.nunique()\n",
    "# df3.groupby('Station_Name')['Corner'].count()\n",
    "# df3[df3.Station_Name==\"Fulton St\"].head(25)\n",
    "# df3.sample(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
